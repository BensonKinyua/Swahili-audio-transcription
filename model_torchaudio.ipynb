{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4daf663-07a1-4c2e-a44c-f10974ed2a78",
   "metadata": {},
   "source": [
    "# Swahili Audio Classification (Zindi Challenge)\n",
    "\n",
    "The following model is used for classifying 12 different swahili words from wav-audiofiles. The audiofiles all have different length and usually silence at the beginning and end. Some have disturbing background noises like other people talking, children playing, clicking sounds...\n",
    "\n",
    "The audiofiles are filtered in length to reduce data to relevant parts. Here, a moving minimum filter is used to filter for long loud sections of the audio files which is usually the spoken word. The file is then cropped to 1,5 seconds around this loud section.\n",
    "\n",
    "Then the audiofile is transformed to a spectrogram. For classification, a pretrained Resnet18 model from the torchvision library is used.\n",
    "\n",
    "**Contents**\n",
    "* Creating a custom data loader for the training data set with preprocessing pipeline\n",
    "* Building a pytorch model\n",
    "* Training the model\n",
    "* Loading the model on the test data and creating the submission file\n",
    "\n",
    "I did not have a gpu available, therefore I set the device to \"**cpu**\".\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7ca89f2-4e7c-4fd5-a01e-08234fcdad41",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mndimage\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m minimum_filter1d\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msampler\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SubsetRandomSampler\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import minimum_filter1d\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchaudio\n",
    "\n",
    "from torch import nn\n",
    "from torchsummary import summary\n",
    "from torchvision.models import resnet18, ResNet18_Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f2f4f1-2e48-479c-8988-eca102de44d8",
   "metadata": {},
   "source": [
    "## Defining variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "1af4c040-d014-4a23-ba7b-869e313da062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data\n",
    "ANNOTATIONS_FILE = \"Train.csv\"\n",
    "AUDIO_DIR = \"Swahili_words\"\n",
    "SAMPLE_RATE = 16000\n",
    "NUM_SAMPLES = int(SAMPLE_RATE*1.5)\n",
    "\n",
    "# Training model\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 12\n",
    "LEARNING_RATE = 0.001\n",
    "VAL_SPLIT = .2\n",
    "SHUFFLE_DATASET = True\n",
    "RANDOM_SEED = 2022\n",
    "\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "898492d4-9042-4ace-8f62-7ea01c361c2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x21671e72550>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set seeds for cpu device\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED) \n",
    "#random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf514a8-8f9b-4737-8282-325f233c79d2",
   "metadata": {},
   "source": [
    "## Custom data loader for Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "74cd91a9-b28a-422c-a563-eac4af835ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwahiliDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom dataset class for loading and transforming the training data\n",
    "    \n",
    "    Input:\n",
    "    * annotations_file\n",
    "    * audio_dir\n",
    "    * transformation\n",
    "    * target_sample_rate\n",
    "    * num_samples\n",
    "    * device\n",
    "    \n",
    "    Returns:\n",
    "    * signal\n",
    "    * label\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 annotations_file,\n",
    "                 audio_dir,\n",
    "                 transformation,\n",
    "                 target_sample_rate,\n",
    "                 num_samples,\n",
    "                 device):\n",
    "        self.annotations = pd.read_csv(annotations_file)\n",
    "        self.audio_dir = audio_dir\n",
    "        self.device = device\n",
    "        self.transformation = transformation\n",
    "        self.target_sample_rate = target_sample_rate\n",
    "        self.num_samples = num_samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        audio_sample_path = self._get_audio_sample_path(index)\n",
    "        label = self._get_audio_sample_label(index)\n",
    "        signal, sr = torchaudio.load(audio_sample_path)\n",
    "        signal = signal.to(self.device)\n",
    "        signal = self._resample_if_necessary(signal, sr)\n",
    "        signal = self._mix_down_if_necessary(signal)\n",
    "        signal = self._cut_if_necessary(signal)\n",
    "        signal = self._right_pad_if_necessary(signal)\n",
    "        signal = self.transformation(signal)\n",
    "        return signal, label\n",
    "\n",
    "\n",
    "    def _resample_if_necessary(self, signal, sr):\n",
    "        \"\"\"\n",
    "        All sounds need to have the same sampling rate.\n",
    "        resampling if the sampling rate differs from the wanted sampling rate.\n",
    "        \"\"\"\n",
    "        if sr != self.target_sample_rate:\n",
    "            resampler = torchaudio.transforms.Resample(sr, self.target_sample_rate)\n",
    "            signal = resampler(signal)\n",
    "        return signal\n",
    "\n",
    "    def _mix_down_if_necessary(self, signal):\n",
    "        \"\"\"\n",
    "        All sounds need to have the same amount of layers.\n",
    "        If a sound was recorded in stereo, it is mixed down to mono.\n",
    "        \"\"\"\n",
    "        if signal.shape[0] > 1:\n",
    "            signal = torch.mean(signal, dim=0, keepdim=True)\n",
    "        return signal\n",
    "    \n",
    "    def _cut_if_necessary(self, signal):\n",
    "        \"\"\"\n",
    "        As most sounds have a silence at the beginning and end and also disturbing sounds like crackling or laughs,\n",
    "        the signal is centered to the loudest part and cut to the wanted length.\n",
    "        The loudest part is found by filtering a moving minimum filter of 1/16th second.\n",
    "        This way, very short loud impulse sounds are removed.\n",
    "        \"\"\"\n",
    "        if signal.shape[1] > self.num_samples:\n",
    "            # Cut first 0.5 seconds, because of signal problems that disturb the mininum_filter1d\n",
    "            signal=signal[:,int(self.target_sample_rate/2):]\n",
    "            # Take the minimum of every 16th second moving filter. \n",
    "            # This filters out short maxima created by impulse noises that might be louder than the speaker.\n",
    "            min_filter = minimum_filter1d(abs(signal), size=int(self.target_sample_rate/16), mode='constant')\n",
    "            ind_max = min_filter[0].argmax()\n",
    "            window_range = int(self.num_samples/2)\n",
    "            if ind_max<=window_range:\n",
    "                ind_lrange=0\n",
    "            else:\n",
    "                ind_lrange=int(ind_max-window_range)\n",
    "\n",
    "            if (signal.shape[1]-ind_max)<=window_range:\n",
    "                ind_rrange=int(signal.shape[1])\n",
    "            else:\n",
    "                ind_rrange=int(ind_max+window_range)\n",
    "            signal=signal[:,ind_lrange:ind_rrange]\n",
    "        return signal\n",
    "\n",
    "    def _right_pad_if_necessary(self, signal):\n",
    "        \"\"\"\n",
    "        Pad the sounds to the same length.\n",
    "        \"\"\"\n",
    "        length_signal = signal.shape[1]\n",
    "        if length_signal < self.num_samples:\n",
    "            num_missing_samples = self.num_samples - length_signal\n",
    "            last_dim_padding = (0, num_missing_samples)\n",
    "            signal = torch.nn.functional.pad(signal, last_dim_padding)\n",
    "        return signal\n",
    "\n",
    "\n",
    "    def _get_audio_sample_path(self, index):\n",
    "        \"\"\"\n",
    "        Path of the audio files.\n",
    "        The individual file names are found in the annotaions file in column 0\n",
    "        \"\"\"\n",
    "        path = os.path.join(self.audio_dir, self.annotations.iloc[index, 0])\n",
    "        return path\n",
    "    \n",
    "    def _get_audio_sample_label(self, index):\n",
    "        \"\"\"\n",
    "        Numerically encoded label.\n",
    "        The target label is found in column 1 of the annotations file. \n",
    "        Column 1 is the swahili word, Column 2 is the english equivalent. \n",
    "        \"\"\"\n",
    "        labels = ['hapana',\n",
    "                  'kumi',\n",
    "                  'mbili',\n",
    "                  'moja',\n",
    "                  'nane',\n",
    "                  'ndio',\n",
    "                  'nne',\n",
    "                  'saba',\n",
    "                  'sita',\n",
    "                  'tano',\n",
    "                  'tatu',\n",
    "                  'tisa']\n",
    "        label = labels.index(self.annotations.iloc[index, 1])\n",
    "        return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "cebfe763-f2ba-4fed-a093-4ab27d38ebff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def audio_transforms_spec(signal):\n",
    "    \"\"\"\n",
    "    Transformations applied to the data at loading.\n",
    "    Audio files are transformed into spectrograms.\n",
    "    The values are transformed to dB scale.\n",
    "    High frequencies are cut off as these are not containing relevant information for spoken words.\n",
    "    \n",
    "    Input: sound signal (amplitude per time)\n",
    "    Returns: spectrogram (frequency per time)\n",
    "    \"\"\"\n",
    "    N_FFT = 1024\n",
    "    \n",
    "    spectrogram = torchaudio.transforms.Spectrogram(\n",
    "        n_fft=N_FFT,\n",
    "        hop_length=100,\n",
    "    )\n",
    "    to_db = torchaudio.transforms.AmplitudeToDB(stype=\"amplitude\", top_db=100)\n",
    "    \n",
    "    spec = spectrogram(signal).to(device)\n",
    "    spec_db = to_db(spec[:,:int(N_FFT/4),:])\n",
    "    return spec_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "cded7c28-9393-4dbc-82d8-04a56fe1b55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def audio_transforms_mel(signal):\n",
    "    \"\"\"\n",
    "    Transformations applied to the data at loading.\n",
    "    Audio files are transformed into spectrograms.\n",
    "    The values are transformed to dB scale.\n",
    "    High frequencies are cut off as these are not containing relevant information for spoken words.\n",
    "    \n",
    "    Input: sound signal (amplitude per time)\n",
    "    Returns: spectrogram (frequency per time)\n",
    "    \"\"\"\n",
    "    mel_spectrogram = torchaudio.transforms.MelSpectrogram(\n",
    "        sample_rate=SAMPLE_RATE,\n",
    "        n_fft=1024,\n",
    "        hop_length=100,\n",
    "        n_mels=200\n",
    "    )\n",
    "    to_db = torchaudio.transforms.AmplitudeToDB(stype=\"amplitude\", top_db=100)\n",
    "    \n",
    "    spec = mel_spectrogram(signal).to(device)\n",
    "    spec_db = to_db(spec)\n",
    "    return spec_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "fea18222-6bdd-45b4-a560-b7d0ccab9e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining which audio transformation function to be used\n",
    "AUDIO_TRANSFORM = audio_transforms_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "f824f743-7baa-4139-84ab-947f23c97f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4200 samples in the dataset.\n"
     ]
    }
   ],
   "source": [
    "# Testing if the dataset class loads the data correctly\n",
    "swahili_train = SwahiliDataset(ANNOTATIONS_FILE,\n",
    "                            AUDIO_DIR,\n",
    "                            AUDIO_TRANSFORM,\n",
    "                            SAMPLE_RATE,\n",
    "                            NUM_SAMPLES,\n",
    "                            device)\n",
    "print(f\"There are {len(swahili_train)} samples in the dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "cef912c2-b606-4c7a-bdb2-7ad9bac7e457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load example data\n",
    "#signal, label = swahili_train[10]\n",
    "#print(f\"example signal: {signal.shape}, label: {label}\")\n",
    "#plt.imshow(signal[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681356e3-bfd9-42ba-a0a4-ed5f2a72718c",
   "metadata": {},
   "source": [
    "## CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "47621921-5e44-49a7-b54c-1f1971c77fd6",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\admin\\Desktop\\project 5\\model_torchaudio.ipynb Cell 17\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/admin/Desktop/project%205/model_torchaudio.ipynb#X16sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Resnet, pretrained\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/admin/Desktop/project%205/model_torchaudio.ipynb#X16sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m model_resnet \u001b[39m=\u001b[39m resnet18(weights\u001b[39m=\u001b[39;49mResNet18_Weights\u001b[39m.\u001b[39;49mDEFAULT)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/admin/Desktop/project%205/model_torchaudio.ipynb#X16sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m model_resnet\u001b[39m.\u001b[39mconv1\u001b[39m=\u001b[39mnn\u001b[39m.\u001b[39mConv2d(\u001b[39m1\u001b[39m, model_resnet\u001b[39m.\u001b[39mconv1\u001b[39m.\u001b[39mout_channels, \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/admin/Desktop/project%205/model_torchaudio.ipynb#X16sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m                       kernel_size\u001b[39m=\u001b[39mmodel_resnet\u001b[39m.\u001b[39mconv1\u001b[39m.\u001b[39mkernel_size[\u001b[39m0\u001b[39m], \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/admin/Desktop/project%205/model_torchaudio.ipynb#X16sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m                       stride\u001b[39m=\u001b[39mmodel_resnet\u001b[39m.\u001b[39mconv1\u001b[39m.\u001b[39mstride[\u001b[39m0\u001b[39m], \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/admin/Desktop/project%205/model_torchaudio.ipynb#X16sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m                       padding\u001b[39m=\u001b[39mmodel_resnet\u001b[39m.\u001b[39mconv1\u001b[39m.\u001b[39mpadding[\u001b[39m0\u001b[39m])\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/admin/Desktop/project%205/model_torchaudio.ipynb#X16sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m num_ftrs \u001b[39m=\u001b[39m model_resnet\u001b[39m.\u001b[39mfc\u001b[39m.\u001b[39min_features\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\learn-env\\lib\\site-packages\\torchvision\\models\\_utils.py:142\u001b[0m, in \u001b[0;36mkwonly_to_pos_or_kw.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    135\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    136\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUsing \u001b[39m\u001b[39m{\u001b[39;00msequence_to_str(\u001b[39mtuple\u001b[39m(keyword_only_kwargs\u001b[39m.\u001b[39mkeys()),\u001b[39m \u001b[39mseparate_last\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mand \u001b[39m\u001b[39m'\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m as positional \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    137\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter(s) is deprecated since 0.13 and may be removed in the future. Please use keyword parameter(s) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    138\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39minstead.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    139\u001b[0m     )\n\u001b[0;32m    140\u001b[0m     kwargs\u001b[39m.\u001b[39mupdate(keyword_only_kwargs)\n\u001b[1;32m--> 142\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\learn-env\\lib\\site-packages\\torchvision\\models\\_utils.py:228\u001b[0m, in \u001b[0;36mhandle_legacy_interface.<locals>.outer_wrapper.<locals>.inner_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    225\u001b[0m     \u001b[39mdel\u001b[39;00m kwargs[pretrained_param]\n\u001b[0;32m    226\u001b[0m     kwargs[weights_param] \u001b[39m=\u001b[39m default_weights_arg\n\u001b[1;32m--> 228\u001b[0m \u001b[39mreturn\u001b[39;00m builder(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\learn-env\\lib\\site-packages\\torchvision\\models\\resnet.py:705\u001b[0m, in \u001b[0;36mresnet18\u001b[1;34m(weights, progress, **kwargs)\u001b[0m\n\u001b[0;32m    685\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"ResNet-18 from `Deep Residual Learning for Image Recognition <https://arxiv.org/pdf/1512.03385.pdf>`__.\u001b[39;00m\n\u001b[0;32m    686\u001b[0m \n\u001b[0;32m    687\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    701\u001b[0m \u001b[39m    :members:\u001b[39;00m\n\u001b[0;32m    702\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    703\u001b[0m weights \u001b[39m=\u001b[39m ResNet18_Weights\u001b[39m.\u001b[39mverify(weights)\n\u001b[1;32m--> 705\u001b[0m \u001b[39mreturn\u001b[39;00m _resnet(BasicBlock, [\u001b[39m2\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m2\u001b[39m], weights, progress, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\learn-env\\lib\\site-packages\\torchvision\\models\\resnet.py:301\u001b[0m, in \u001b[0;36m_resnet\u001b[1;34m(block, layers, weights, progress, **kwargs)\u001b[0m\n\u001b[0;32m    298\u001b[0m model \u001b[39m=\u001b[39m ResNet(block, layers, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    300\u001b[0m \u001b[39mif\u001b[39;00m weights \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 301\u001b[0m     model\u001b[39m.\u001b[39mload_state_dict(weights\u001b[39m.\u001b[39;49mget_state_dict(progress\u001b[39m=\u001b[39;49mprogress))\n\u001b[0;32m    303\u001b[0m \u001b[39mreturn\u001b[39;00m model\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\learn-env\\lib\\site-packages\\torchvision\\models\\_api.py:89\u001b[0m, in \u001b[0;36mWeightsEnum.get_state_dict\u001b[1;34m(self, progress)\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_state_dict\u001b[39m(\u001b[39mself\u001b[39m, progress: \u001b[39mbool\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Mapping[\u001b[39mstr\u001b[39m, Any]:\n\u001b[1;32m---> 89\u001b[0m     \u001b[39mreturn\u001b[39;00m load_state_dict_from_url(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49murl, progress\u001b[39m=\u001b[39;49mprogress)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\learn-env\\lib\\site-packages\\torch\\hub.py:750\u001b[0m, in \u001b[0;36mload_state_dict_from_url\u001b[1;34m(url, model_dir, map_location, progress, check_hash, file_name)\u001b[0m\n\u001b[0;32m    748\u001b[0m \u001b[39mif\u001b[39;00m _is_legacy_zip_format(cached_file):\n\u001b[0;32m    749\u001b[0m     \u001b[39mreturn\u001b[39;00m _legacy_zip_load(cached_file, model_dir, map_location)\n\u001b[1;32m--> 750\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mload(cached_file, map_location\u001b[39m=\u001b[39;49mmap_location)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\learn-env\\lib\\site-packages\\torch\\serialization.py:809\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[0;32m    807\u001b[0m             \u001b[39mexcept\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    808\u001b[0m                 \u001b[39mraise\u001b[39;00m pickle\u001b[39m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(e)) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 809\u001b[0m         \u001b[39mreturn\u001b[39;00m _load(opened_zipfile, map_location, pickle_module, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n\u001b[0;32m    810\u001b[0m \u001b[39mif\u001b[39;00m weights_only:\n\u001b[0;32m    811\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\learn-env\\lib\\site-packages\\torch\\serialization.py:1172\u001b[0m, in \u001b[0;36m_load\u001b[1;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1170\u001b[0m unpickler \u001b[39m=\u001b[39m UnpicklerWrapper(data_file, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n\u001b[0;32m   1171\u001b[0m unpickler\u001b[39m.\u001b[39mpersistent_load \u001b[39m=\u001b[39m persistent_load\n\u001b[1;32m-> 1172\u001b[0m result \u001b[39m=\u001b[39m unpickler\u001b[39m.\u001b[39;49mload()\n\u001b[0;32m   1174\u001b[0m torch\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[0;32m   1176\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\learn-env\\lib\\site-packages\\torch\\_utils.py:169\u001b[0m, in \u001b[0;36m_rebuild_tensor_v2\u001b[1;34m(storage, storage_offset, size, stride, requires_grad, backward_hooks, metadata)\u001b[0m\n\u001b[0;32m    166\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_rebuild_tensor_v2\u001b[39m(\n\u001b[0;32m    167\u001b[0m     storage, storage_offset, size, stride, requires_grad, backward_hooks, metadata\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m\n\u001b[0;32m    168\u001b[0m ):\n\u001b[1;32m--> 169\u001b[0m     tensor \u001b[39m=\u001b[39m _rebuild_tensor(storage, storage_offset, size, stride)\n\u001b[0;32m    170\u001b[0m     tensor\u001b[39m.\u001b[39mrequires_grad \u001b[39m=\u001b[39m requires_grad\n\u001b[0;32m    171\u001b[0m     \u001b[39mif\u001b[39;00m metadata:\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\learn-env\\lib\\site-packages\\torch\\_utils.py:148\u001b[0m, in \u001b[0;36m_rebuild_tensor\u001b[1;34m(storage, storage_offset, size, stride)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_rebuild_tensor\u001b[39m(storage, storage_offset, size, stride):\n\u001b[0;32m    146\u001b[0m     \u001b[39m# first construct a tensor with the correct dtype/device\u001b[39;00m\n\u001b[0;32m    147\u001b[0m     t \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([], dtype\u001b[39m=\u001b[39mstorage\u001b[39m.\u001b[39mdtype, device\u001b[39m=\u001b[39mstorage\u001b[39m.\u001b[39m_untyped_storage\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m--> 148\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39;49mset_(storage\u001b[39m.\u001b[39;49m_untyped_storage, storage_offset, size, stride)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Resnet, pretrained\n",
    "model_resnet = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "model_resnet.conv1=nn.Conv2d(1, model_resnet.conv1.out_channels, \n",
    "                      kernel_size=model_resnet.conv1.kernel_size[0], \n",
    "                      stride=model_resnet.conv1.stride[0], \n",
    "                      padding=model_resnet.conv1.padding[0])\n",
    "num_ftrs = model_resnet.fc.in_features\n",
    "model_resnet.fc = nn.Linear(num_ftrs, 12) # 12 output classes for 12 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e344da-e10c-4799-bc50-9a1edf87aa7a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 100, 121]           3,200\n",
      "       BatchNorm2d-2         [-1, 64, 100, 121]             128\n",
      "              ReLU-3         [-1, 64, 100, 121]               0\n",
      "         MaxPool2d-4           [-1, 64, 50, 61]               0\n",
      "            Conv2d-5           [-1, 64, 50, 61]          36,864\n",
      "       BatchNorm2d-6           [-1, 64, 50, 61]             128\n",
      "              ReLU-7           [-1, 64, 50, 61]               0\n",
      "            Conv2d-8           [-1, 64, 50, 61]          36,864\n",
      "       BatchNorm2d-9           [-1, 64, 50, 61]             128\n",
      "             ReLU-10           [-1, 64, 50, 61]               0\n",
      "       BasicBlock-11           [-1, 64, 50, 61]               0\n",
      "           Conv2d-12           [-1, 64, 50, 61]          36,864\n",
      "      BatchNorm2d-13           [-1, 64, 50, 61]             128\n",
      "             ReLU-14           [-1, 64, 50, 61]               0\n",
      "           Conv2d-15           [-1, 64, 50, 61]          36,864\n",
      "      BatchNorm2d-16           [-1, 64, 50, 61]             128\n",
      "             ReLU-17           [-1, 64, 50, 61]               0\n",
      "       BasicBlock-18           [-1, 64, 50, 61]               0\n",
      "           Conv2d-19          [-1, 128, 25, 31]          73,728\n",
      "      BatchNorm2d-20          [-1, 128, 25, 31]             256\n",
      "             ReLU-21          [-1, 128, 25, 31]               0\n",
      "           Conv2d-22          [-1, 128, 25, 31]         147,456\n",
      "      BatchNorm2d-23          [-1, 128, 25, 31]             256\n",
      "           Conv2d-24          [-1, 128, 25, 31]           8,192\n",
      "      BatchNorm2d-25          [-1, 128, 25, 31]             256\n",
      "             ReLU-26          [-1, 128, 25, 31]               0\n",
      "       BasicBlock-27          [-1, 128, 25, 31]               0\n",
      "           Conv2d-28          [-1, 128, 25, 31]         147,456\n",
      "      BatchNorm2d-29          [-1, 128, 25, 31]             256\n",
      "             ReLU-30          [-1, 128, 25, 31]               0\n",
      "           Conv2d-31          [-1, 128, 25, 31]         147,456\n",
      "      BatchNorm2d-32          [-1, 128, 25, 31]             256\n",
      "             ReLU-33          [-1, 128, 25, 31]               0\n",
      "       BasicBlock-34          [-1, 128, 25, 31]               0\n",
      "           Conv2d-35          [-1, 256, 13, 16]         294,912\n",
      "      BatchNorm2d-36          [-1, 256, 13, 16]             512\n",
      "             ReLU-37          [-1, 256, 13, 16]               0\n",
      "           Conv2d-38          [-1, 256, 13, 16]         589,824\n",
      "      BatchNorm2d-39          [-1, 256, 13, 16]             512\n",
      "           Conv2d-40          [-1, 256, 13, 16]          32,768\n",
      "      BatchNorm2d-41          [-1, 256, 13, 16]             512\n",
      "             ReLU-42          [-1, 256, 13, 16]               0\n",
      "       BasicBlock-43          [-1, 256, 13, 16]               0\n",
      "           Conv2d-44          [-1, 256, 13, 16]         589,824\n",
      "      BatchNorm2d-45          [-1, 256, 13, 16]             512\n",
      "             ReLU-46          [-1, 256, 13, 16]               0\n",
      "           Conv2d-47          [-1, 256, 13, 16]         589,824\n",
      "      BatchNorm2d-48          [-1, 256, 13, 16]             512\n",
      "             ReLU-49          [-1, 256, 13, 16]               0\n",
      "       BasicBlock-50          [-1, 256, 13, 16]               0\n",
      "           Conv2d-51            [-1, 512, 7, 8]       1,179,648\n",
      "      BatchNorm2d-52            [-1, 512, 7, 8]           1,024\n",
      "             ReLU-53            [-1, 512, 7, 8]               0\n",
      "           Conv2d-54            [-1, 512, 7, 8]       2,359,296\n",
      "      BatchNorm2d-55            [-1, 512, 7, 8]           1,024\n",
      "           Conv2d-56            [-1, 512, 7, 8]         131,072\n",
      "      BatchNorm2d-57            [-1, 512, 7, 8]           1,024\n",
      "             ReLU-58            [-1, 512, 7, 8]               0\n",
      "       BasicBlock-59            [-1, 512, 7, 8]               0\n",
      "           Conv2d-60            [-1, 512, 7, 8]       2,359,296\n",
      "      BatchNorm2d-61            [-1, 512, 7, 8]           1,024\n",
      "             ReLU-62            [-1, 512, 7, 8]               0\n",
      "           Conv2d-63            [-1, 512, 7, 8]       2,359,296\n",
      "      BatchNorm2d-64            [-1, 512, 7, 8]           1,024\n",
      "             ReLU-65            [-1, 512, 7, 8]               0\n",
      "       BasicBlock-66            [-1, 512, 7, 8]               0\n",
      "AdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0\n",
      "           Linear-68                   [-1, 12]           6,156\n",
      "================================================================\n",
      "Total params: 11,176,460\n",
      "Trainable params: 11,176,460\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.18\n",
      "Forward/backward pass size (MB): 62.18\n",
      "Params size (MB): 42.63\n",
      "Estimated Total Size (MB): 105.00\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "resnet = model_resnet.to(device)\n",
    "summary(resnet, (1, 200, 241)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c52f9b4-9317-4259-a3b1-6f5c5b8416af",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1bcedf-6c4f-43b0-960f-11524091f46b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_data_loaders(train_data, batch_size, val_split, shuffle_dataset, random_seed):\n",
    "    \"\"\"\n",
    "    Data Loader for training and validation set from the same custom Dataset for trainings data.\n",
    "    \n",
    "    Input:\n",
    "    train_data (Custom Dataset with signal and label)\n",
    "    batch_size\n",
    "    val_split (split size)\n",
    "    shuffle_dataset (bool, if dataset is shuffled for train/val-split)\n",
    "    random_seed\n",
    "    \n",
    "    Returns:\n",
    "    train_dataloader\n",
    "    val_dataloader\n",
    "    \"\"\"\n",
    "    dataset_size = len(train_data)\n",
    "    indices = list(range(dataset_size))\n",
    "    split = int(np.floor(val_split * dataset_size))\n",
    "    if shuffle_dataset:\n",
    "        np.random.seed(random_seed)\n",
    "        np.random.shuffle(indices)\n",
    "    train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "    # Creating data samplers and loaders:\n",
    "    train_sampler = SubsetRandomSampler(train_indices)\n",
    "    val_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, \n",
    "                                               sampler=train_sampler)\n",
    "    val_dataloader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n",
    "                                                    sampler=val_sampler)\n",
    "    \n",
    "    return train_dataloader, val_dataloader\n",
    "\n",
    "\n",
    "def train_single_epoch(model, train_data_loader, val_data_loader, loss_fn, optimiser, scheduler, device):\n",
    "    for input, target in train_data_loader:\n",
    "        input, target = input.to(device), target.to(device)\n",
    "\n",
    "        # calculate loss\n",
    "        train_prediction = model(input)\n",
    "        train_loss = loss_fn(train_prediction, target)\n",
    "\n",
    "        # backpropagate error and update weights\n",
    "        optimiser.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimiser.step()\n",
    "\n",
    "    \n",
    "    for input, target in val_data_loader:\n",
    "        input, target = input.to(device), target.to(device)\n",
    "\n",
    "        # calculate loss\n",
    "        val_prediction = model(input)\n",
    "        val_loss = loss_fn(val_prediction, target)\n",
    "    \n",
    "    \n",
    "    scheduler.step()\n",
    "    print(f\"training loss: {train_loss.item()}, validation loss: {val_loss.item()}\")\n",
    "\n",
    "\n",
    "def train(model, train_data_loader, val_data_loader, loss_fn, optimiser, scheduler, device, epochs):\n",
    "    for i in range(epochs):\n",
    "        print(f\"Epoch {i+1}\")\n",
    "        train_single_epoch(model, train_data_loader, val_data_loader, loss_fn, optimiser, scheduler, device)\n",
    "        print(\"---------------------------\")\n",
    "    print(\"Finished training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea886e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "swahili_train = SwahiliDataset(ANNOTATIONS_FILE,\n",
    "                               AUDIO_DIR,\n",
    "                               AUDIO_TRANSFORM,\n",
    "                               SAMPLE_RATE,\n",
    "                               NUM_SAMPLES,\n",
    "                               device)\n",
    "    \n",
    "train_dataloader, val_dataloader = create_data_loaders(swahili_train, \n",
    "                                                       BATCH_SIZE, \n",
    "                                                       VAL_SPLIT, \n",
    "                                                       SHUFFLE_DATASET, \n",
    "                                                       RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44676376-4a79-467c-8b39-2f86d89a2828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 1\n",
      "Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "training loss: 0.3916754126548767, validation loss: 0.3244917690753937\n",
      "---------------------------\n",
      "Epoch 2\n",
      "Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "training loss: 0.8984215259552002, validation loss: 0.2993735074996948\n",
      "---------------------------\n",
      "Epoch 3\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "training loss: 0.02049178071320057, validation loss: 0.1827593445777893\n",
      "---------------------------\n",
      "Epoch 4\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "training loss: 0.21344296634197235, validation loss: 0.26339825987815857\n",
      "---------------------------\n",
      "Epoch 5\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "training loss: 0.23281297087669373, validation loss: 0.2980828583240509\n",
      "---------------------------\n",
      "Epoch 6\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "training loss: 0.1664021909236908, validation loss: 0.20852941274642944\n",
      "---------------------------\n",
      "Epoch 7\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "training loss: 0.08190638571977615, validation loss: 0.299240380525589\n",
      "---------------------------\n",
      "Epoch 8\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "training loss: 0.006990799214690924, validation loss: 0.22431731224060059\n",
      "---------------------------\n",
      "Epoch 9\n",
      "Adjusting learning rate of group 0 to 1.0000e-06.\n",
      "training loss: 0.07457125931978226, validation loss: 0.18415993452072144\n",
      "---------------------------\n",
      "Epoch 10\n",
      "Adjusting learning rate of group 0 to 1.0000e-06.\n",
      "training loss: 0.08161585032939911, validation loss: 0.19286268949508667\n",
      "---------------------------\n",
      "Epoch 11\n",
      "Adjusting learning rate of group 0 to 1.0000e-06.\n",
      "training loss: 0.04868970811367035, validation loss: 0.1472277045249939\n",
      "---------------------------\n",
      "Epoch 12\n",
      "Adjusting learning rate of group 0 to 1.0000e-07.\n",
      "training loss: 0.040052954107522964, validation loss: 0.13784630596637726\n",
      "---------------------------\n",
      "Finished training\n",
      "Trained model saved at resnet_v2_spec.pth\n"
     ]
    }
   ],
   "source": [
    "swahili_train = SwahiliDataset(ANNOTATIONS_FILE,\n",
    "                               AUDIO_DIR,\n",
    "                               AUDIO_TRANSFORM,\n",
    "                               SAMPLE_RATE,\n",
    "                               NUM_SAMPLES,\n",
    "                               device)\n",
    "    \n",
    "train_dataloader, val_dataloader = create_data_loaders(swahili_train, \n",
    "                                                       BATCH_SIZE, \n",
    "                                                       VAL_SPLIT, \n",
    "                                                       SHUFFLE_DATASET, \n",
    "                                                       RANDOM_SEED)\n",
    "\n",
    "# construct model and assign it to device\n",
    "resnet = model_resnet.to(device)\n",
    "\n",
    "# initialise loss funtion + optimiser\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimiser = torch.optim.Adam(resnet.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimiser, step_size=3, gamma=0.1, verbose=True)\n",
    "\n",
    "# train model\n",
    "train(resnet, train_dataloader, val_dataloader, loss_fn, optimiser, scheduler, device, EPOCHS)\n",
    "\n",
    "# save model\n",
    "torch.save(resnet.state_dict(), \"resnet_v2_spec.pth\")\n",
    "print(\"Trained model saved at resnet_v2_spec.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bb4fff-afab-4f79-8b54-db9b8502cf22",
   "metadata": {},
   "source": [
    "## Prediction on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415b7593-9722-4af5-9598-31830379c0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwahiliDataset_Testset(SwahiliDataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset for test data.\n",
    "    The label is not known for the test data, therefore the Dataset for training data cannot be used.\n",
    "    This Dataset inherits the functions defined in SwahiliDataset.\n",
    "    The same transformations are applied to the signal.\n",
    "    The getitem function is adjusted to return only the signal.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 annotations_file,\n",
    "                 audio_dir,\n",
    "                 transformation,\n",
    "                 target_sample_rate,\n",
    "                 num_samples,\n",
    "                 device):\n",
    "        self.annotations = pd.read_csv(annotations_file)\n",
    "        self.audio_dir = audio_dir\n",
    "        self.device = device\n",
    "        self.transformation = transformation\n",
    "        self.target_sample_rate = target_sample_rate\n",
    "        self.num_samples = num_samples\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        audio_sample_path = self._get_audio_sample_path(index)\n",
    "        signal, sr = torchaudio.load(audio_sample_path)\n",
    "        signal = signal.to(self.device)\n",
    "        signal = self._resample_if_necessary(signal, sr)\n",
    "        signal = self._mix_down_if_necessary(signal)\n",
    "        signal = self._cut_if_necessary(signal)\n",
    "        signal = self._right_pad_if_necessary(signal)\n",
    "        signal = self.transformation(signal)\n",
    "        return signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e903b1-bf6d-4b94-8d75-867da2b35b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_testset(model, data_loader):\n",
    "    \"\"\"\n",
    "    Predictions on the test set\n",
    "    Returns:\n",
    "    predictions with values between 0 and 1 due to added softmax layer.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for input in data_loader:\n",
    "            input = input.to(device)\n",
    "            predictions = nn.Softmax(dim=1)(model(input))\n",
    "        # Tensor (1, 10) -> [ [0.1, 0.01, ..., 0.6] ]\n",
    "    return predictions\n",
    "\n",
    "def create_test_data_loader(test_data):\n",
    "    \"\"\"\n",
    "    test data loader with full test data set as batch\n",
    "    \"\"\"\n",
    "    dataset_size = len(test_data)\n",
    "    test_dataloader = DataLoader(test_data, batch_size=dataset_size)\n",
    "    return test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bf83ca-5d97-4807-8284-af90a68a3245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: tensor([[4.1770e-06, 1.5575e-06, 1.4050e-06,  ..., 2.7642e-07, 5.9579e-06,\n",
      "         9.3024e-04],\n",
      "        [8.6177e-06, 2.3448e-06, 4.1617e-06,  ..., 1.2969e-06, 3.5505e-05,\n",
      "         9.9955e-01],\n",
      "        [7.7253e-03, 3.5064e-03, 2.0147e-02,  ..., 2.6538e-03, 1.1227e-02,\n",
      "         7.9924e-01],\n",
      "        ...,\n",
      "        [9.8910e-01, 1.5456e-04, 2.0553e-04,  ..., 7.3204e-04, 2.0548e-03,\n",
      "         3.6964e-04],\n",
      "        [3.3280e-06, 2.3386e-06, 2.0560e-06,  ..., 1.0990e-06, 5.7111e-06,\n",
      "         9.9990e-01],\n",
      "        [1.4899e-05, 1.2027e-06, 3.0611e-06,  ..., 2.0842e-06, 3.2444e-05,\n",
      "         2.2035e-05]])\n"
     ]
    }
   ],
   "source": [
    "# load back the model\n",
    "resnet = model_resnet.to(device)\n",
    "state_dict = torch.load(\"resnet_v2_spec.pth\")\n",
    "resnet.load_state_dict(state_dict)\n",
    "\n",
    "TEST_ANNOTATIONS_FILE = \"data/Test.csv\"\n",
    "\n",
    "swahili_test = SwahiliDataset_Testset(TEST_ANNOTATIONS_FILE,\n",
    "                                      AUDIO_DIR,\n",
    "                                      AUDIO_TRANSFORM,\n",
    "                                      SAMPLE_RATE,\n",
    "                                      NUM_SAMPLES,\n",
    "                                      \"cpu\")\n",
    "\n",
    "test_dataloader = create_test_data_loader(swahili_test)\n",
    "\n",
    "predicted = predict_testset(resnet, test_dataloader)\n",
    "print(f\"Predicted: {predicted}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917f1a07-45db-4d7f-b582-865edf7a1281",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1800, 12])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26be2dfb",
   "metadata": {},
   "source": [
    "## AlexNet Model \n",
    "\n",
    "This model is pretrained on the ImageNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba93a79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\anaconda3\\envs\\learn-env\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\admin\\anaconda3\\envs\\learn-env\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "# Load the pre-trained AlexNet model\n",
    "model_alexnet = models.alexnet(pretrained=True)\n",
    "\n",
    "# Modify the first convolutional layer for a single input channel\n",
    "model_alexnet.features[0] = nn.Conv2d(1, model_alexnet.features[0].out_channels, \n",
    "                      kernel_size=model_alexnet.features[0].kernel_size[0], \n",
    "                      stride=model_alexnet.features[0].stride[0], \n",
    "                      padding=model_alexnet.features[0].padding[0])\n",
    "\n",
    "# Replace the last fully connected layer for your specific number of output classes\n",
    "num_ftrs = model_alexnet.classifier[6].in_features\n",
    "model_alexnet.classifier[6] = nn.Linear(num_ftrs, 12) # 12 output classes for 12 words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538a002d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 55, 55]           7,808\n",
      "              ReLU-2           [-1, 64, 55, 55]               0\n",
      "         MaxPool2d-3           [-1, 64, 27, 27]               0\n",
      "            Conv2d-4          [-1, 192, 27, 27]         307,392\n",
      "              ReLU-5          [-1, 192, 27, 27]               0\n",
      "         MaxPool2d-6          [-1, 192, 13, 13]               0\n",
      "            Conv2d-7          [-1, 384, 13, 13]         663,936\n",
      "              ReLU-8          [-1, 384, 13, 13]               0\n",
      "            Conv2d-9          [-1, 256, 13, 13]         884,992\n",
      "             ReLU-10          [-1, 256, 13, 13]               0\n",
      "           Conv2d-11          [-1, 256, 13, 13]         590,080\n",
      "             ReLU-12          [-1, 256, 13, 13]               0\n",
      "        MaxPool2d-13            [-1, 256, 6, 6]               0\n",
      "AdaptiveAvgPool2d-14            [-1, 256, 6, 6]               0\n",
      "          Dropout-15                 [-1, 9216]               0\n",
      "           Linear-16                 [-1, 4096]      37,752,832\n",
      "             ReLU-17                 [-1, 4096]               0\n",
      "          Dropout-18                 [-1, 4096]               0\n",
      "           Linear-19                 [-1, 4096]      16,781,312\n",
      "             ReLU-20                 [-1, 4096]               0\n",
      "           Linear-21                   [-1, 12]          49,164\n",
      "================================================================\n",
      "Total params: 57,037,516\n",
      "Trainable params: 57,037,516\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.19\n",
      "Forward/backward pass size (MB): 8.37\n",
      "Params size (MB): 217.58\n",
      "Estimated Total Size (MB): 226.14\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# AlexNet, pretrained\n",
    "alexnet=model_alexnet.to(device)\n",
    "summary(alexnet, (1, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295f662c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 1\n",
      "Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "training loss: 2.494018316268921, validation loss: 2.501676082611084\n",
      "---------------------------\n",
      "Epoch 2\n",
      "Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "training loss: 2.494164228439331, validation loss: 2.4760847091674805\n",
      "---------------------------\n",
      "Epoch 3\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "training loss: 2.4852044582366943, validation loss: 2.494711399078369\n",
      "---------------------------\n",
      "Epoch 4\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "training loss: 2.486607789993286, validation loss: 2.481253147125244\n",
      "---------------------------\n",
      "Epoch 5\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "training loss: 2.4892990589141846, validation loss: 2.4898455142974854\n",
      "---------------------------\n",
      "Epoch 6\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "training loss: 2.4904024600982666, validation loss: 2.4872446060180664\n",
      "---------------------------\n",
      "Epoch 7\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "training loss: 2.4810163974761963, validation loss: 2.486618995666504\n",
      "---------------------------\n",
      "Epoch 8\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "training loss: 2.487931251525879, validation loss: 2.4885549545288086\n",
      "---------------------------\n",
      "Epoch 9\n",
      "Adjusting learning rate of group 0 to 1.0000e-06.\n",
      "training loss: 2.478339195251465, validation loss: 2.4811837673187256\n",
      "---------------------------\n",
      "Epoch 10\n",
      "Adjusting learning rate of group 0 to 1.0000e-06.\n",
      "training loss: 2.483893871307373, validation loss: 2.4893054962158203\n",
      "---------------------------\n",
      "Epoch 11\n",
      "Adjusting learning rate of group 0 to 1.0000e-06.\n",
      "training loss: 2.496518611907959, validation loss: 2.4825680255889893\n",
      "---------------------------\n",
      "Epoch 12\n",
      "Adjusting learning rate of group 0 to 1.0000e-07.\n",
      "training loss: 2.478318691253662, validation loss: 2.4882893562316895\n",
      "---------------------------\n",
      "Finished training\n",
      "Trained model saved at resnet_v2_spec.pth\n"
     ]
    }
   ],
   "source": [
    "swahili_train = SwahiliDataset(ANNOTATIONS_FILE,\n",
    "                               AUDIO_DIR,\n",
    "                               AUDIO_TRANSFORM,\n",
    "                               SAMPLE_RATE,\n",
    "                               NUM_SAMPLES,\n",
    "                               device)\n",
    "    \n",
    "train_dataloader, val_dataloader = create_data_loaders(swahili_train, \n",
    "                                                       BATCH_SIZE, \n",
    "                                                       VAL_SPLIT, \n",
    "                                                       SHUFFLE_DATASET, \n",
    "                                                       RANDOM_SEED)\n",
    "\n",
    "# construct model and assign it to device\n",
    "alexnet=model_alexnet.to(device)\n",
    "\n",
    "# initialise loss funtion + optimiser\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimiser = torch.optim.Adam(alexnet.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimiser, step_size=3, gamma=0.1, verbose=True)\n",
    "\n",
    "# train model\n",
    "train(alexnet, train_dataloader, val_dataloader, loss_fn, optimiser, scheduler, device, EPOCHS)\n",
    "\n",
    "# save model\n",
    "torch.save(alexnet.state_dict(), \"alexnet_v2_spec.pth\")\n",
    "print(\"Trained model saved at alexnet_v2_spec.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dca3460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# make predictions\n",
    "TEST_ANNOTATIONS_FILE = \"Test.csv\"\n",
    "# Load the test dataset\n",
    "swahili_test = SwahiliDataset_Testset(TEST_ANNOTATIONS_FILE, AUDIO_DIR, AUDIO_TRANSFORM, SAMPLE_RATE, NUM_SAMPLES, device)\n",
    "\n",
    "# Create a dataloader for the test dataset\n",
    "test_dataloader = torch.utils.data.DataLoader(swahili_test, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "alexnet.eval()\n",
    "\n",
    "# Iterate over the test dataloader and make predictions\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for inputs in test_dataloader:\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = alexnet(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        predictions.extend(predicted.tolist())\n",
    "\n",
    "# Print the predictions\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63aa179a",
   "metadata": {},
   "source": [
    "## inception model\n",
    "\n",
    "This model is pretrained on the ImageNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a9e343",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\anaconda3\\envs\\learn-env\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\admin\\anaconda3\\envs\\learn-env\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 32, 149, 149]             864\n",
      "       BatchNorm2d-2         [-1, 32, 149, 149]              64\n",
      "       BasicConv2d-3         [-1, 32, 149, 149]               0\n",
      "            Conv2d-4         [-1, 32, 147, 147]           9,216\n",
      "       BatchNorm2d-5         [-1, 32, 147, 147]              64\n",
      "       BasicConv2d-6         [-1, 32, 147, 147]               0\n",
      "            Conv2d-7         [-1, 64, 147, 147]          18,432\n",
      "       BatchNorm2d-8         [-1, 64, 147, 147]             128\n",
      "       BasicConv2d-9         [-1, 64, 147, 147]               0\n",
      "        MaxPool2d-10           [-1, 64, 73, 73]               0\n",
      "           Conv2d-11           [-1, 80, 73, 73]           5,120\n",
      "      BatchNorm2d-12           [-1, 80, 73, 73]             160\n",
      "      BasicConv2d-13           [-1, 80, 73, 73]               0\n",
      "           Conv2d-14          [-1, 192, 71, 71]         138,240\n",
      "      BatchNorm2d-15          [-1, 192, 71, 71]             384\n",
      "      BasicConv2d-16          [-1, 192, 71, 71]               0\n",
      "        MaxPool2d-17          [-1, 192, 35, 35]               0\n",
      "           Conv2d-18           [-1, 64, 35, 35]          12,288\n",
      "      BatchNorm2d-19           [-1, 64, 35, 35]             128\n",
      "      BasicConv2d-20           [-1, 64, 35, 35]               0\n",
      "           Conv2d-21           [-1, 48, 35, 35]           9,216\n",
      "      BatchNorm2d-22           [-1, 48, 35, 35]              96\n",
      "      BasicConv2d-23           [-1, 48, 35, 35]               0\n",
      "           Conv2d-24           [-1, 64, 35, 35]          76,800\n",
      "      BatchNorm2d-25           [-1, 64, 35, 35]             128\n",
      "      BasicConv2d-26           [-1, 64, 35, 35]               0\n",
      "           Conv2d-27           [-1, 64, 35, 35]          12,288\n",
      "      BatchNorm2d-28           [-1, 64, 35, 35]             128\n",
      "      BasicConv2d-29           [-1, 64, 35, 35]               0\n",
      "           Conv2d-30           [-1, 96, 35, 35]          55,296\n",
      "      BatchNorm2d-31           [-1, 96, 35, 35]             192\n",
      "      BasicConv2d-32           [-1, 96, 35, 35]               0\n",
      "           Conv2d-33           [-1, 96, 35, 35]          82,944\n",
      "      BatchNorm2d-34           [-1, 96, 35, 35]             192\n",
      "      BasicConv2d-35           [-1, 96, 35, 35]               0\n",
      "           Conv2d-36           [-1, 32, 35, 35]           6,144\n",
      "      BatchNorm2d-37           [-1, 32, 35, 35]              64\n",
      "      BasicConv2d-38           [-1, 32, 35, 35]               0\n",
      "       InceptionA-39          [-1, 256, 35, 35]               0\n",
      "           Conv2d-40           [-1, 64, 35, 35]          16,384\n",
      "      BatchNorm2d-41           [-1, 64, 35, 35]             128\n",
      "      BasicConv2d-42           [-1, 64, 35, 35]               0\n",
      "           Conv2d-43           [-1, 48, 35, 35]          12,288\n",
      "      BatchNorm2d-44           [-1, 48, 35, 35]              96\n",
      "      BasicConv2d-45           [-1, 48, 35, 35]               0\n",
      "           Conv2d-46           [-1, 64, 35, 35]          76,800\n",
      "      BatchNorm2d-47           [-1, 64, 35, 35]             128\n",
      "      BasicConv2d-48           [-1, 64, 35, 35]               0\n",
      "           Conv2d-49           [-1, 64, 35, 35]          16,384\n",
      "      BatchNorm2d-50           [-1, 64, 35, 35]             128\n",
      "      BasicConv2d-51           [-1, 64, 35, 35]               0\n",
      "           Conv2d-52           [-1, 96, 35, 35]          55,296\n",
      "      BatchNorm2d-53           [-1, 96, 35, 35]             192\n",
      "      BasicConv2d-54           [-1, 96, 35, 35]               0\n",
      "           Conv2d-55           [-1, 96, 35, 35]          82,944\n",
      "      BatchNorm2d-56           [-1, 96, 35, 35]             192\n",
      "      BasicConv2d-57           [-1, 96, 35, 35]               0\n",
      "           Conv2d-58           [-1, 64, 35, 35]          16,384\n",
      "      BatchNorm2d-59           [-1, 64, 35, 35]             128\n",
      "      BasicConv2d-60           [-1, 64, 35, 35]               0\n",
      "       InceptionA-61          [-1, 288, 35, 35]               0\n",
      "           Conv2d-62           [-1, 64, 35, 35]          18,432\n",
      "      BatchNorm2d-63           [-1, 64, 35, 35]             128\n",
      "      BasicConv2d-64           [-1, 64, 35, 35]               0\n",
      "           Conv2d-65           [-1, 48, 35, 35]          13,824\n",
      "      BatchNorm2d-66           [-1, 48, 35, 35]              96\n",
      "      BasicConv2d-67           [-1, 48, 35, 35]               0\n",
      "           Conv2d-68           [-1, 64, 35, 35]          76,800\n",
      "      BatchNorm2d-69           [-1, 64, 35, 35]             128\n",
      "      BasicConv2d-70           [-1, 64, 35, 35]               0\n",
      "           Conv2d-71           [-1, 64, 35, 35]          18,432\n",
      "      BatchNorm2d-72           [-1, 64, 35, 35]             128\n",
      "      BasicConv2d-73           [-1, 64, 35, 35]               0\n",
      "           Conv2d-74           [-1, 96, 35, 35]          55,296\n",
      "      BatchNorm2d-75           [-1, 96, 35, 35]             192\n",
      "      BasicConv2d-76           [-1, 96, 35, 35]               0\n",
      "           Conv2d-77           [-1, 96, 35, 35]          82,944\n",
      "      BatchNorm2d-78           [-1, 96, 35, 35]             192\n",
      "      BasicConv2d-79           [-1, 96, 35, 35]               0\n",
      "           Conv2d-80           [-1, 64, 35, 35]          18,432\n",
      "      BatchNorm2d-81           [-1, 64, 35, 35]             128\n",
      "      BasicConv2d-82           [-1, 64, 35, 35]               0\n",
      "       InceptionA-83          [-1, 288, 35, 35]               0\n",
      "           Conv2d-84          [-1, 384, 17, 17]         995,328\n",
      "      BatchNorm2d-85          [-1, 384, 17, 17]             768\n",
      "      BasicConv2d-86          [-1, 384, 17, 17]               0\n",
      "           Conv2d-87           [-1, 64, 35, 35]          18,432\n",
      "      BatchNorm2d-88           [-1, 64, 35, 35]             128\n",
      "      BasicConv2d-89           [-1, 64, 35, 35]               0\n",
      "           Conv2d-90           [-1, 96, 35, 35]          55,296\n",
      "      BatchNorm2d-91           [-1, 96, 35, 35]             192\n",
      "      BasicConv2d-92           [-1, 96, 35, 35]               0\n",
      "           Conv2d-93           [-1, 96, 17, 17]          82,944\n",
      "      BatchNorm2d-94           [-1, 96, 17, 17]             192\n",
      "      BasicConv2d-95           [-1, 96, 17, 17]               0\n",
      "       InceptionB-96          [-1, 768, 17, 17]               0\n",
      "           Conv2d-97          [-1, 192, 17, 17]         147,456\n",
      "      BatchNorm2d-98          [-1, 192, 17, 17]             384\n",
      "      BasicConv2d-99          [-1, 192, 17, 17]               0\n",
      "          Conv2d-100          [-1, 128, 17, 17]          98,304\n",
      "     BatchNorm2d-101          [-1, 128, 17, 17]             256\n",
      "     BasicConv2d-102          [-1, 128, 17, 17]               0\n",
      "          Conv2d-103          [-1, 128, 17, 17]         114,688\n",
      "     BatchNorm2d-104          [-1, 128, 17, 17]             256\n",
      "     BasicConv2d-105          [-1, 128, 17, 17]               0\n",
      "          Conv2d-106          [-1, 192, 17, 17]         172,032\n",
      "     BatchNorm2d-107          [-1, 192, 17, 17]             384\n",
      "     BasicConv2d-108          [-1, 192, 17, 17]               0\n",
      "          Conv2d-109          [-1, 128, 17, 17]          98,304\n",
      "     BatchNorm2d-110          [-1, 128, 17, 17]             256\n",
      "     BasicConv2d-111          [-1, 128, 17, 17]               0\n",
      "          Conv2d-112          [-1, 128, 17, 17]         114,688\n",
      "     BatchNorm2d-113          [-1, 128, 17, 17]             256\n",
      "     BasicConv2d-114          [-1, 128, 17, 17]               0\n",
      "          Conv2d-115          [-1, 128, 17, 17]         114,688\n",
      "     BatchNorm2d-116          [-1, 128, 17, 17]             256\n",
      "     BasicConv2d-117          [-1, 128, 17, 17]               0\n",
      "          Conv2d-118          [-1, 128, 17, 17]         114,688\n",
      "     BatchNorm2d-119          [-1, 128, 17, 17]             256\n",
      "     BasicConv2d-120          [-1, 128, 17, 17]               0\n",
      "          Conv2d-121          [-1, 192, 17, 17]         172,032\n",
      "     BatchNorm2d-122          [-1, 192, 17, 17]             384\n",
      "     BasicConv2d-123          [-1, 192, 17, 17]               0\n",
      "          Conv2d-124          [-1, 192, 17, 17]         147,456\n",
      "     BatchNorm2d-125          [-1, 192, 17, 17]             384\n",
      "     BasicConv2d-126          [-1, 192, 17, 17]               0\n",
      "      InceptionC-127          [-1, 768, 17, 17]               0\n",
      "          Conv2d-128          [-1, 192, 17, 17]         147,456\n",
      "     BatchNorm2d-129          [-1, 192, 17, 17]             384\n",
      "     BasicConv2d-130          [-1, 192, 17, 17]               0\n",
      "          Conv2d-131          [-1, 160, 17, 17]         122,880\n",
      "     BatchNorm2d-132          [-1, 160, 17, 17]             320\n",
      "     BasicConv2d-133          [-1, 160, 17, 17]               0\n",
      "          Conv2d-134          [-1, 160, 17, 17]         179,200\n",
      "     BatchNorm2d-135          [-1, 160, 17, 17]             320\n",
      "     BasicConv2d-136          [-1, 160, 17, 17]               0\n",
      "          Conv2d-137          [-1, 192, 17, 17]         215,040\n",
      "     BatchNorm2d-138          [-1, 192, 17, 17]             384\n",
      "     BasicConv2d-139          [-1, 192, 17, 17]               0\n",
      "          Conv2d-140          [-1, 160, 17, 17]         122,880\n",
      "     BatchNorm2d-141          [-1, 160, 17, 17]             320\n",
      "     BasicConv2d-142          [-1, 160, 17, 17]               0\n",
      "          Conv2d-143          [-1, 160, 17, 17]         179,200\n",
      "     BatchNorm2d-144          [-1, 160, 17, 17]             320\n",
      "     BasicConv2d-145          [-1, 160, 17, 17]               0\n",
      "          Conv2d-146          [-1, 160, 17, 17]         179,200\n",
      "     BatchNorm2d-147          [-1, 160, 17, 17]             320\n",
      "     BasicConv2d-148          [-1, 160, 17, 17]               0\n",
      "          Conv2d-149          [-1, 160, 17, 17]         179,200\n",
      "     BatchNorm2d-150          [-1, 160, 17, 17]             320\n",
      "     BasicConv2d-151          [-1, 160, 17, 17]               0\n",
      "          Conv2d-152          [-1, 192, 17, 17]         215,040\n",
      "     BatchNorm2d-153          [-1, 192, 17, 17]             384\n",
      "     BasicConv2d-154          [-1, 192, 17, 17]               0\n",
      "          Conv2d-155          [-1, 192, 17, 17]         147,456\n",
      "     BatchNorm2d-156          [-1, 192, 17, 17]             384\n",
      "     BasicConv2d-157          [-1, 192, 17, 17]               0\n",
      "      InceptionC-158          [-1, 768, 17, 17]               0\n",
      "          Conv2d-159          [-1, 192, 17, 17]         147,456\n",
      "     BatchNorm2d-160          [-1, 192, 17, 17]             384\n",
      "     BasicConv2d-161          [-1, 192, 17, 17]               0\n",
      "          Conv2d-162          [-1, 160, 17, 17]         122,880\n",
      "     BatchNorm2d-163          [-1, 160, 17, 17]             320\n",
      "     BasicConv2d-164          [-1, 160, 17, 17]               0\n",
      "          Conv2d-165          [-1, 160, 17, 17]         179,200\n",
      "     BatchNorm2d-166          [-1, 160, 17, 17]             320\n",
      "     BasicConv2d-167          [-1, 160, 17, 17]               0\n",
      "          Conv2d-168          [-1, 192, 17, 17]         215,040\n",
      "     BatchNorm2d-169          [-1, 192, 17, 17]             384\n",
      "     BasicConv2d-170          [-1, 192, 17, 17]               0\n",
      "          Conv2d-171          [-1, 160, 17, 17]         122,880\n",
      "     BatchNorm2d-172          [-1, 160, 17, 17]             320\n",
      "     BasicConv2d-173          [-1, 160, 17, 17]               0\n",
      "          Conv2d-174          [-1, 160, 17, 17]         179,200\n",
      "     BatchNorm2d-175          [-1, 160, 17, 17]             320\n",
      "     BasicConv2d-176          [-1, 160, 17, 17]               0\n",
      "          Conv2d-177          [-1, 160, 17, 17]         179,200\n",
      "     BatchNorm2d-178          [-1, 160, 17, 17]             320\n",
      "     BasicConv2d-179          [-1, 160, 17, 17]               0\n",
      "          Conv2d-180          [-1, 160, 17, 17]         179,200\n",
      "     BatchNorm2d-181          [-1, 160, 17, 17]             320\n",
      "     BasicConv2d-182          [-1, 160, 17, 17]               0\n",
      "          Conv2d-183          [-1, 192, 17, 17]         215,040\n",
      "     BatchNorm2d-184          [-1, 192, 17, 17]             384\n",
      "     BasicConv2d-185          [-1, 192, 17, 17]               0\n",
      "          Conv2d-186          [-1, 192, 17, 17]         147,456\n",
      "     BatchNorm2d-187          [-1, 192, 17, 17]             384\n",
      "     BasicConv2d-188          [-1, 192, 17, 17]               0\n",
      "      InceptionC-189          [-1, 768, 17, 17]               0\n",
      "          Conv2d-190          [-1, 192, 17, 17]         147,456\n",
      "     BatchNorm2d-191          [-1, 192, 17, 17]             384\n",
      "     BasicConv2d-192          [-1, 192, 17, 17]               0\n",
      "          Conv2d-193          [-1, 192, 17, 17]         147,456\n",
      "     BatchNorm2d-194          [-1, 192, 17, 17]             384\n",
      "     BasicConv2d-195          [-1, 192, 17, 17]               0\n",
      "          Conv2d-196          [-1, 192, 17, 17]         258,048\n",
      "     BatchNorm2d-197          [-1, 192, 17, 17]             384\n",
      "     BasicConv2d-198          [-1, 192, 17, 17]               0\n",
      "          Conv2d-199          [-1, 192, 17, 17]         258,048\n",
      "     BatchNorm2d-200          [-1, 192, 17, 17]             384\n",
      "     BasicConv2d-201          [-1, 192, 17, 17]               0\n",
      "          Conv2d-202          [-1, 192, 17, 17]         147,456\n",
      "     BatchNorm2d-203          [-1, 192, 17, 17]             384\n",
      "     BasicConv2d-204          [-1, 192, 17, 17]               0\n",
      "          Conv2d-205          [-1, 192, 17, 17]         258,048\n",
      "     BatchNorm2d-206          [-1, 192, 17, 17]             384\n",
      "     BasicConv2d-207          [-1, 192, 17, 17]               0\n",
      "          Conv2d-208          [-1, 192, 17, 17]         258,048\n",
      "     BatchNorm2d-209          [-1, 192, 17, 17]             384\n",
      "     BasicConv2d-210          [-1, 192, 17, 17]               0\n",
      "          Conv2d-211          [-1, 192, 17, 17]         258,048\n",
      "     BatchNorm2d-212          [-1, 192, 17, 17]             384\n",
      "     BasicConv2d-213          [-1, 192, 17, 17]               0\n",
      "          Conv2d-214          [-1, 192, 17, 17]         258,048\n",
      "     BatchNorm2d-215          [-1, 192, 17, 17]             384\n",
      "     BasicConv2d-216          [-1, 192, 17, 17]               0\n",
      "          Conv2d-217          [-1, 192, 17, 17]         147,456\n",
      "     BatchNorm2d-218          [-1, 192, 17, 17]             384\n",
      "     BasicConv2d-219          [-1, 192, 17, 17]               0\n",
      "      InceptionC-220          [-1, 768, 17, 17]               0\n",
      "          Conv2d-221            [-1, 128, 5, 5]          98,304\n",
      "     BatchNorm2d-222            [-1, 128, 5, 5]             256\n",
      "     BasicConv2d-223            [-1, 128, 5, 5]               0\n",
      "          Conv2d-224            [-1, 768, 1, 1]       2,457,600\n",
      "     BatchNorm2d-225            [-1, 768, 1, 1]           1,536\n",
      "     BasicConv2d-226            [-1, 768, 1, 1]               0\n",
      "          Linear-227                 [-1, 1000]         769,000\n",
      "    InceptionAux-228                 [-1, 1000]               0\n",
      "          Conv2d-229          [-1, 192, 17, 17]         147,456\n",
      "     BatchNorm2d-230          [-1, 192, 17, 17]             384\n",
      "     BasicConv2d-231          [-1, 192, 17, 17]               0\n",
      "          Conv2d-232            [-1, 320, 8, 8]         552,960\n",
      "     BatchNorm2d-233            [-1, 320, 8, 8]             640\n",
      "     BasicConv2d-234            [-1, 320, 8, 8]               0\n",
      "          Conv2d-235          [-1, 192, 17, 17]         147,456\n",
      "     BatchNorm2d-236          [-1, 192, 17, 17]             384\n",
      "     BasicConv2d-237          [-1, 192, 17, 17]               0\n",
      "          Conv2d-238          [-1, 192, 17, 17]         258,048\n",
      "     BatchNorm2d-239          [-1, 192, 17, 17]             384\n",
      "     BasicConv2d-240          [-1, 192, 17, 17]               0\n",
      "          Conv2d-241          [-1, 192, 17, 17]         258,048\n",
      "     BatchNorm2d-242          [-1, 192, 17, 17]             384\n",
      "     BasicConv2d-243          [-1, 192, 17, 17]               0\n",
      "          Conv2d-244            [-1, 192, 8, 8]         331,776\n",
      "     BatchNorm2d-245            [-1, 192, 8, 8]             384\n",
      "     BasicConv2d-246            [-1, 192, 8, 8]               0\n",
      "      InceptionD-247           [-1, 1280, 8, 8]               0\n",
      "          Conv2d-248            [-1, 320, 8, 8]         409,600\n",
      "     BatchNorm2d-249            [-1, 320, 8, 8]             640\n",
      "     BasicConv2d-250            [-1, 320, 8, 8]               0\n",
      "          Conv2d-251            [-1, 384, 8, 8]         491,520\n",
      "     BatchNorm2d-252            [-1, 384, 8, 8]             768\n",
      "     BasicConv2d-253            [-1, 384, 8, 8]               0\n",
      "          Conv2d-254            [-1, 384, 8, 8]         442,368\n",
      "     BatchNorm2d-255            [-1, 384, 8, 8]             768\n",
      "     BasicConv2d-256            [-1, 384, 8, 8]               0\n",
      "          Conv2d-257            [-1, 384, 8, 8]         442,368\n",
      "     BatchNorm2d-258            [-1, 384, 8, 8]             768\n",
      "     BasicConv2d-259            [-1, 384, 8, 8]               0\n",
      "          Conv2d-260            [-1, 448, 8, 8]         573,440\n",
      "     BatchNorm2d-261            [-1, 448, 8, 8]             896\n",
      "     BasicConv2d-262            [-1, 448, 8, 8]               0\n",
      "          Conv2d-263            [-1, 384, 8, 8]       1,548,288\n",
      "     BatchNorm2d-264            [-1, 384, 8, 8]             768\n",
      "     BasicConv2d-265            [-1, 384, 8, 8]               0\n",
      "          Conv2d-266            [-1, 384, 8, 8]         442,368\n",
      "     BatchNorm2d-267            [-1, 384, 8, 8]             768\n",
      "     BasicConv2d-268            [-1, 384, 8, 8]               0\n",
      "          Conv2d-269            [-1, 384, 8, 8]         442,368\n",
      "     BatchNorm2d-270            [-1, 384, 8, 8]             768\n",
      "     BasicConv2d-271            [-1, 384, 8, 8]               0\n",
      "          Conv2d-272            [-1, 192, 8, 8]         245,760\n",
      "     BatchNorm2d-273            [-1, 192, 8, 8]             384\n",
      "     BasicConv2d-274            [-1, 192, 8, 8]               0\n",
      "      InceptionE-275           [-1, 2048, 8, 8]               0\n",
      "          Conv2d-276            [-1, 320, 8, 8]         655,360\n",
      "     BatchNorm2d-277            [-1, 320, 8, 8]             640\n",
      "     BasicConv2d-278            [-1, 320, 8, 8]               0\n",
      "          Conv2d-279            [-1, 384, 8, 8]         786,432\n",
      "     BatchNorm2d-280            [-1, 384, 8, 8]             768\n",
      "     BasicConv2d-281            [-1, 384, 8, 8]               0\n",
      "          Conv2d-282            [-1, 384, 8, 8]         442,368\n",
      "     BatchNorm2d-283            [-1, 384, 8, 8]             768\n",
      "     BasicConv2d-284            [-1, 384, 8, 8]               0\n",
      "          Conv2d-285            [-1, 384, 8, 8]         442,368\n",
      "     BatchNorm2d-286            [-1, 384, 8, 8]             768\n",
      "     BasicConv2d-287            [-1, 384, 8, 8]               0\n",
      "          Conv2d-288            [-1, 448, 8, 8]         917,504\n",
      "     BatchNorm2d-289            [-1, 448, 8, 8]             896\n",
      "     BasicConv2d-290            [-1, 448, 8, 8]               0\n",
      "          Conv2d-291            [-1, 384, 8, 8]       1,548,288\n",
      "     BatchNorm2d-292            [-1, 384, 8, 8]             768\n",
      "     BasicConv2d-293            [-1, 384, 8, 8]               0\n",
      "          Conv2d-294            [-1, 384, 8, 8]         442,368\n",
      "     BatchNorm2d-295            [-1, 384, 8, 8]             768\n",
      "     BasicConv2d-296            [-1, 384, 8, 8]               0\n",
      "          Conv2d-297            [-1, 384, 8, 8]         442,368\n",
      "     BatchNorm2d-298            [-1, 384, 8, 8]             768\n",
      "     BasicConv2d-299            [-1, 384, 8, 8]               0\n",
      "          Conv2d-300            [-1, 192, 8, 8]         393,216\n",
      "     BatchNorm2d-301            [-1, 192, 8, 8]             384\n",
      "     BasicConv2d-302            [-1, 192, 8, 8]               0\n",
      "      InceptionE-303           [-1, 2048, 8, 8]               0\n",
      "AdaptiveAvgPool2d-304           [-1, 2048, 1, 1]               0\n",
      "         Dropout-305           [-1, 2048, 1, 1]               0\n",
      "          Linear-306                   [-1, 12]          24,588\n",
      "      Inception3-307     [[-1, 12], [-1, 1000]]               0\n",
      "================================================================\n",
      "Total params: 25,136,852\n",
      "Trainable params: 25,136,852\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 1.02\n",
      "Forward/backward pass size (MB): 228.56\n",
      "Params size (MB): 95.89\n",
      "Estimated Total Size (MB): 325.47\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torchsummary import summary\n",
    "\n",
    "# Create a modified Inception model\n",
    "class ModifiedInception(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ModifiedInception, self).__init__()\n",
    "        self.inception = models.inception_v3(pretrained=True, aux_logits=True)\n",
    "        # Modify the last fully connected layer for your specific number of output classes\n",
    "        self.inception.fc = nn.Linear(2048, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.inception(x)\n",
    "\n",
    "# Initialize the model\n",
    "num_classes = 12  # Replace with the number of output classes you have\n",
    "inception = ModifiedInception(num_classes).to(device)\n",
    "\n",
    "# Print the model summary with an example input shape\n",
    "summary(inception, (3, 299, 299))  # Use (3, 299, 299) for color images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea2c933",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Input image tensor should have at least 3 dimensions, but found 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\admin\\Desktop\\project 5\\model_torchaudio.ipynb Cell 35\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/admin/Desktop/project%205/model_torchaudio.ipynb#X64sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m         accuracy \u001b[39m=\u001b[39m correct \u001b[39m/\u001b[39m total\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/admin/Desktop/project%205/model_torchaudio.ipynb#X64sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mValidation accuracy: \u001b[39m\u001b[39m{\u001b[39;00maccuracy\u001b[39m \u001b[39m\u001b[39m*\u001b[39m\u001b[39m \u001b[39m\u001b[39m100\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m%\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/admin/Desktop/project%205/model_torchaudio.ipynb#X64sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m train_epoch(inception, train_dataloader, loss_fn, optimiser, device)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/admin/Desktop/project%205/model_torchaudio.ipynb#X64sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m val_epoch(inception, val_dataloader, loss_fn, device)\n",
      "\u001b[1;32mc:\\Users\\admin\\Desktop\\project 5\\model_torchaudio.ipynb Cell 35\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/admin/Desktop/project%205/model_torchaudio.ipynb#X64sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain_epoch\u001b[39m(model, train_data_loader, loss_fn, optimiser, device):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/admin/Desktop/project%205/model_torchaudio.ipynb#X64sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     model\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/admin/Desktop/project%205/model_torchaudio.ipynb#X64sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39mfor\u001b[39;00m \u001b[39minput\u001b[39m, target \u001b[39min\u001b[39;00m train_data_loader:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/admin/Desktop/project%205/model_torchaudio.ipynb#X64sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m         \u001b[39minput\u001b[39m, target \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mto(device), target\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/admin/Desktop/project%205/model_torchaudio.ipynb#X64sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m         optimiser\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\learn-env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\learn-env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    675\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    676\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 677\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    678\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    679\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\learn-env\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\learn-env\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "\u001b[1;32mc:\\Users\\admin\\Desktop\\project 5\\model_torchaudio.ipynb Cell 35\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/admin/Desktop/project%205/model_torchaudio.ipynb#X64sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m signal \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cut_if_necessary(signal)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/admin/Desktop/project%205/model_torchaudio.ipynb#X64sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m signal \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_right_pad_if_necessary(signal)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/admin/Desktop/project%205/model_torchaudio.ipynb#X64sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m signal \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformation(signal)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/admin/Desktop/project%205/model_torchaudio.ipynb#X64sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m \u001b[39mreturn\u001b[39;00m signal, label\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\learn-env\\lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[39m=\u001b[39m t(img)\n\u001b[0;32m     96\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\learn-env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\learn-env\\lib\\site-packages\\torchvision\\transforms\\transforms.py:1580\u001b[0m, in \u001b[0;36mGrayscale.forward\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m   1572\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[0;32m   1573\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1574\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   1575\u001b[0m \u001b[39m        img (PIL Image or Tensor): Image to be converted to grayscale.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1578\u001b[0m \u001b[39m        PIL Image or Tensor: Grayscaled image.\u001b[39;00m\n\u001b[0;32m   1579\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1580\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mrgb_to_grayscale(img, num_output_channels\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_output_channels)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\learn-env\\lib\\site-packages\\torchvision\\transforms\\functional.py:1299\u001b[0m, in \u001b[0;36mrgb_to_grayscale\u001b[1;34m(img, num_output_channels)\u001b[0m\n\u001b[0;32m   1296\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(img, torch\u001b[39m.\u001b[39mTensor):\n\u001b[0;32m   1297\u001b[0m     \u001b[39mreturn\u001b[39;00m F_pil\u001b[39m.\u001b[39mto_grayscale(img, num_output_channels)\n\u001b[1;32m-> 1299\u001b[0m \u001b[39mreturn\u001b[39;00m F_t\u001b[39m.\u001b[39;49mrgb_to_grayscale(img, num_output_channels)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\learn-env\\lib\\site-packages\\torchvision\\transforms\\_functional_tensor.py:148\u001b[0m, in \u001b[0;36mrgb_to_grayscale\u001b[1;34m(img, num_output_channels)\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrgb_to_grayscale\u001b[39m(img: Tensor, num_output_channels: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m    147\u001b[0m     \u001b[39mif\u001b[39;00m img\u001b[39m.\u001b[39mndim \u001b[39m<\u001b[39m \u001b[39m3\u001b[39m:\n\u001b[1;32m--> 148\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInput image tensor should have at least 3 dimensions, but found \u001b[39m\u001b[39m{\u001b[39;00mimg\u001b[39m.\u001b[39mndim\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    149\u001b[0m     _assert_channels(img, [\u001b[39m1\u001b[39m, \u001b[39m3\u001b[39m])\n\u001b[0;32m    151\u001b[0m     \u001b[39mif\u001b[39;00m num_output_channels \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m (\u001b[39m1\u001b[39m, \u001b[39m3\u001b[39m):\n",
      "\u001b[1;31mTypeError\u001b[0m: Input image tensor should have at least 3 dimensions, but found 2"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "\n",
    "def train(model, train_data_loader, val_data_loader, loss_fn, optimiser, scheduler, device, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch + 1} of {epochs}\")\n",
    "        train_epoch(model, train_data_loader, loss_fn, optimiser, device)\n",
    "        print(\"---------------------------\")\n",
    "        val_epoch(model, val_data_loader, loss_fn, device)\n",
    "        scheduler.step()\n",
    "\n",
    "def train_epoch(model, train_data_loader, loss_fn, optimiser, device):\n",
    "    model.train()\n",
    "    for input, target in train_data_loader:\n",
    "        input, target = input.to(device), target.to(device)\n",
    "        optimiser.zero_grad()\n",
    "        output = model(input)\n",
    "        loss = loss_fn(output, target)\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "\n",
    "def val_epoch(model, val_data_loader, loss_fn, device):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for input, target in val_data_loader:\n",
    "            input, target = input.to(device), target.to(device)\n",
    "            output = model(input)\n",
    "            loss = loss_fn(output, target)\n",
    "            correct += (output.argmax(1) == target).sum().item()\n",
    "            total += target.size(0)\n",
    "        accuracy = correct / total\n",
    "        print(f\"Validation accuracy: {accuracy * 100}%\")\n",
    "\n",
    "\n",
    "train_epoch(inception, train_dataloader, loss_fn, optimiser, device)\n",
    "\n",
    "val_epoch(inception, val_dataloader, loss_fn, device)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debe5ebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\anaconda3\\envs\\learn-env\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\admin\\anaconda3\\envs\\learn-env\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 1 of 12\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Input image tensor should have at least 3 dimensions, but found 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\admin\\Desktop\\project 5\\model_torchaudio.ipynb Cell 36\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/admin/Desktop/project%205/model_torchaudio.ipynb#X63sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m scheduler \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mlr_scheduler\u001b[39m.\u001b[39mStepLR(optimiser, step_size\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m, gamma\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m, verbose\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/admin/Desktop/project%205/model_torchaudio.ipynb#X63sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39m# Train the model using the train and validation data loaders\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/admin/Desktop/project%205/model_torchaudio.ipynb#X63sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m train(inception, train_dataloader, val_dataloader, loss_fn, optimiser, scheduler, device, EPOCHS)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/admin/Desktop/project%205/model_torchaudio.ipynb#X63sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39m# Save the trained model\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/admin/Desktop/project%205/model_torchaudio.ipynb#X63sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m torch\u001b[39m.\u001b[39msave(inception\u001b[39m.\u001b[39mstate_dict(), \u001b[39m\"\u001b[39m\u001b[39minception_v2_spec.pth\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32mc:\\Users\\admin\\Desktop\\project 5\\model_torchaudio.ipynb Cell 36\u001b[0m line \u001b[0;36m6\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/admin/Desktop/project%205/model_torchaudio.ipynb#X63sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/admin/Desktop/project%205/model_torchaudio.ipynb#X63sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m \u001b[39m\u001b[39m+\u001b[39m\u001b[39m \u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m of \u001b[39m\u001b[39m{\u001b[39;00mepochs\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/admin/Desktop/project%205/model_torchaudio.ipynb#X63sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     train_epoch(model, train_data_loader, loss_fn, optimiser, device)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/admin/Desktop/project%205/model_torchaudio.ipynb#X63sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m---------------------------\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/admin/Desktop/project%205/model_torchaudio.ipynb#X63sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     val_epoch(model, val_data_loader, loss_fn, device)\n",
      "\u001b[1;32mc:\\Users\\admin\\Desktop\\project 5\\model_torchaudio.ipynb Cell 36\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/admin/Desktop/project%205/model_torchaudio.ipynb#X63sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain_epoch\u001b[39m(model, train_data_loader, loss_fn, optimiser, device):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/admin/Desktop/project%205/model_torchaudio.ipynb#X63sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     model\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/admin/Desktop/project%205/model_torchaudio.ipynb#X63sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39mfor\u001b[39;00m \u001b[39minput\u001b[39m, target \u001b[39min\u001b[39;00m train_data_loader:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/admin/Desktop/project%205/model_torchaudio.ipynb#X63sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m         \u001b[39minput\u001b[39m, target \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mto(device), target\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/admin/Desktop/project%205/model_torchaudio.ipynb#X63sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m         optimiser\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\learn-env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\learn-env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    675\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    676\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 677\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    678\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    679\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\learn-env\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\learn-env\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "\u001b[1;32mc:\\Users\\admin\\Desktop\\project 5\\model_torchaudio.ipynb Cell 36\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/admin/Desktop/project%205/model_torchaudio.ipynb#X63sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m signal \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cut_if_necessary(signal)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/admin/Desktop/project%205/model_torchaudio.ipynb#X63sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m signal \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_right_pad_if_necessary(signal)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/admin/Desktop/project%205/model_torchaudio.ipynb#X63sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m signal \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformation(signal)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/admin/Desktop/project%205/model_torchaudio.ipynb#X63sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m \u001b[39mreturn\u001b[39;00m signal, label\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\learn-env\\lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[39m=\u001b[39m t(img)\n\u001b[0;32m     96\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\learn-env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\learn-env\\lib\\site-packages\\torchvision\\transforms\\transforms.py:1580\u001b[0m, in \u001b[0;36mGrayscale.forward\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m   1572\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[0;32m   1573\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1574\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   1575\u001b[0m \u001b[39m        img (PIL Image or Tensor): Image to be converted to grayscale.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1578\u001b[0m \u001b[39m        PIL Image or Tensor: Grayscaled image.\u001b[39;00m\n\u001b[0;32m   1579\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1580\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mrgb_to_grayscale(img, num_output_channels\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_output_channels)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\learn-env\\lib\\site-packages\\torchvision\\transforms\\functional.py:1299\u001b[0m, in \u001b[0;36mrgb_to_grayscale\u001b[1;34m(img, num_output_channels)\u001b[0m\n\u001b[0;32m   1296\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(img, torch\u001b[39m.\u001b[39mTensor):\n\u001b[0;32m   1297\u001b[0m     \u001b[39mreturn\u001b[39;00m F_pil\u001b[39m.\u001b[39mto_grayscale(img, num_output_channels)\n\u001b[1;32m-> 1299\u001b[0m \u001b[39mreturn\u001b[39;00m F_t\u001b[39m.\u001b[39;49mrgb_to_grayscale(img, num_output_channels)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\learn-env\\lib\\site-packages\\torchvision\\transforms\\_functional_tensor.py:148\u001b[0m, in \u001b[0;36mrgb_to_grayscale\u001b[1;34m(img, num_output_channels)\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrgb_to_grayscale\u001b[39m(img: Tensor, num_output_channels: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m    147\u001b[0m     \u001b[39mif\u001b[39;00m img\u001b[39m.\u001b[39mndim \u001b[39m<\u001b[39m \u001b[39m3\u001b[39m:\n\u001b[1;32m--> 148\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInput image tensor should have at least 3 dimensions, but found \u001b[39m\u001b[39m{\u001b[39;00mimg\u001b[39m.\u001b[39mndim\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    149\u001b[0m     _assert_channels(img, [\u001b[39m1\u001b[39m, \u001b[39m3\u001b[39m])\n\u001b[0;32m    151\u001b[0m     \u001b[39mif\u001b[39;00m num_output_channels \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m (\u001b[39m1\u001b[39m, \u001b[39m3\u001b[39m):\n",
      "\u001b[1;31mTypeError\u001b[0m: Input image tensor should have at least 3 dimensions, but found 2"
     ]
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "# Modify your audio transform to ensure it generates three-channel images\n",
    "AUDIO_TRANSFORM = transforms.Compose([\n",
    "    # Your existing transforms here\n",
    "    transforms.Grayscale(num_output_channels=3),  # Convert to RGB\n",
    "    transforms.ToTensor(),  # Convert the image to a tensor\n",
    "])\n",
    "\n",
    "# Create the SwahiliDataset with the modified transform for both training and validation\n",
    "swahili_train = SwahiliDataset(ANNOTATIONS_FILE,\n",
    "                               AUDIO_DIR,\n",
    "                               AUDIO_TRANSFORM,\n",
    "                               SAMPLE_RATE,\n",
    "                               NUM_SAMPLES,\n",
    "                               device)\n",
    "\n",
    "# Create data loaders for both training and validation\n",
    "train_dataloader, val_dataloader = create_data_loaders(swahili_train, \n",
    "                                                       BATCH_SIZE, \n",
    "                                                       VAL_SPLIT, \n",
    "                                                       SHUFFLE_DATASET, \n",
    "                                                       RANDOM_SEED)\n",
    "\n",
    "\n",
    "# construct model and assign it to device\n",
    "inception = ModifiedInception(num_classes).to(device)\n",
    "\n",
    "# initialise loss funtion + optimiser\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimiser = torch.optim.Adam(inception.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimiser, step_size=3, gamma=0.1, verbose=True)\n",
    "\n",
    "# Train the model using the train and validation data loaders\n",
    "train(inception, train_dataloader, val_dataloader, loss_fn, optimiser, scheduler, device, EPOCHS)\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(inception.state_dict(), \"inception_v2_spec.pth\")\n",
    "print(\"Trained model saved at inception_v2_spec.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088669cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model\n",
    "\n",
    "def calculate_accuracy(predicted, true_labels):\n",
    "    predicted_labels = predicted.argmax(dim=1)\n",
    "    accuracy = (predicted_labels == true_labels).float().mean()\n",
    "    return accuracy\n",
    "\n",
    "def calculate_precision(predicted, true_labels):\n",
    "    predicted_labels = predicted.argmax(dim=1)\n",
    "    precision = (predicted_labels == true_labels).float().mean()\n",
    "    return precision\n",
    "\n",
    "def calculate_recall(predicted, true_labels):\n",
    "    predicted_labels = predicted.argmax(dim=1)\n",
    "    recall = (predicted_labels == true_labels).float().mean()\n",
    "    return recall\n",
    "\n",
    "def calculate_f1_score(predicted, true_labels):\n",
    "    predicted_labels = predicted.argmax(dim=1)\n",
    "    f1_score = (predicted_labels == true_labels).float().mean()\n",
    "    return f1_score\n",
    "\n",
    "# load back the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee68b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model evaluation\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    # Iterate over the test data loader\n",
    "    for input in test_dataloader:\n",
    "        input = input.to(device)  # Move the input to the device (e.g. GPU)\n",
    "        predictions = model(input)  # Forward pass\n",
    "        # Perform any necessary post-processing on the predictions\n",
    "\n",
    "# Add your code to calculate evaluation metrics or perform any other evaluation tasks\n",
    "# Calculate evaluation metrics\n",
    "accuracy = calculate_accuracy(predicted, true_labels)\n",
    "precision = calculate_precision(predicted, true_labels)\n",
    "recall = calculate_recall(predicted, true_labels)\n",
    "f1_score = calculate_f1_score(predicted, true_labels)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443f660d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e0135e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "               RNN-1  [[-1, 128, 32], [-1, 2, 32]]               0\n",
      "            Linear-2                    [-1, 2]              66\n",
      "================================================================\n",
      "Total params: 66\n",
      "Trainable params: 66\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 2.00\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 2.01\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "RNN: Expected input to be 2-D or 3-D but received 4-D tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\admin\\Desktop\\project 5\\model_torchaudio.ipynb Cell 27\u001b[0m line \u001b[0;36m8\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/admin/Desktop/project%205/model_torchaudio.ipynb#X41sZmlsZQ%3D%3D?line=73'>74</a>\u001b[0m labels \u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/admin/Desktop/project%205/model_torchaudio.ipynb#X41sZmlsZQ%3D%3D?line=75'>76</a>\u001b[0m \u001b[39m# Check the original shape of the inputs (batch_size, sequence_length, input_size)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/admin/Desktop/project%205/model_torchaudio.ipynb#X41sZmlsZQ%3D%3D?line=76'>77</a>\u001b[0m \u001b[39m# You may need to adjust the permutation dimensions based on your actual input shape\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/admin/Desktop/project%205/model_torchaudio.ipynb#X41sZmlsZQ%3D%3D?line=77'>78</a>\u001b[0m \u001b[39m#inputs = inputs.squeeze(-4,-3)\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/admin/Desktop/project%205/model_torchaudio.ipynb#X41sZmlsZQ%3D%3D?line=79'>80</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(inputs)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/admin/Desktop/project%205/model_torchaudio.ipynb#X41sZmlsZQ%3D%3D?line=80'>81</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/admin/Desktop/project%205/model_torchaudio.ipynb#X41sZmlsZQ%3D%3D?line=81'>82</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\learn-env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\Users\\admin\\Desktop\\project 5\\model_torchaudio.ipynb Cell 27\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/admin/Desktop/project%205/model_torchaudio.ipynb#X41sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m h0 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers, x\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhidden_size)\u001b[39m.\u001b[39mto(x\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/admin/Desktop/project%205/model_torchaudio.ipynb#X41sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m# Forward pass through RNN layer\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/admin/Desktop/project%205/model_torchaudio.ipynb#X41sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m out, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrnn(x, h0)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/admin/Desktop/project%205/model_torchaudio.ipynb#X41sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39m# Index to get the output at the last time step\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/admin/Desktop/project%205/model_torchaudio.ipynb#X41sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc(out[:, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, :])\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\learn-env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\learn-env\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:476\u001b[0m, in \u001b[0;36mRNN.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    474\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    475\u001b[0m     batch_sizes \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 476\u001b[0m     \u001b[39massert\u001b[39;00m (\u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdim() \u001b[39min\u001b[39;00m (\u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m)), \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mRNN: Expected input to be 2-D or 3-D but received \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdim()\u001b[39m}\u001b[39;00m\u001b[39m-D tensor\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    477\u001b[0m     is_batched \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdim() \u001b[39m==\u001b[39m \u001b[39m3\u001b[39m\n\u001b[0;32m    478\u001b[0m     batch_dim \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_first \u001b[39melse\u001b[39;00m \u001b[39m1\u001b[39m\n",
      "\u001b[1;31mAssertionError\u001b[0m: RNN: Expected input to be 2-D or 3-D but received 4-D tensor"
     ]
    }
   ],
   "source": [
    "# rnn model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CustomRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(CustomRNN, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Define the RNN layer\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        \n",
    "        # Define the fully connected layer for output\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state with zeros\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        # Forward pass through RNN layer\n",
    "        out, _ = self.rnn(x, h0)\n",
    "        \n",
    "        # Index to get the output at the last time step\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        \n",
    "        return out\n",
    "\n",
    "# Example usage:\n",
    "input_size = 10\n",
    "hidden_size = 32\n",
    "num_layers = 2\n",
    "output_size = 2\n",
    "\n",
    "model = CustomRNN(input_size, hidden_size, num_layers, output_size)\n",
    "\n",
    "sequence_length = 10\n",
    "\n",
    "\n",
    "# Create a random input sequence\n",
    "input_data = torch.randn(sequence_length, input_size)\n",
    "\n",
    "# Reshape for batch processing if needed (optional)\n",
    "input_data = input_data.unsqueeze(0)  # Add a batch dimension\n",
    "\n",
    "# Example input\n",
    "\n",
    "x = torch.randn(1, 10, 10)\n",
    "x = x[:, :, :input_size]\n",
    "\n",
    "# Forward pass\n",
    "out = model(x)\n",
    "\n",
    "# Output shape\n",
    "from torchsummary import summary\n",
    "\n",
    "# Define your model\n",
    "model = CustomRNN(input_size, hidden_size, num_layers, output_size)\n",
    "\n",
    "# Print the summary of the model\n",
    "summary(model, input_size=(BATCH_SIZE, input_size))\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "model = CustomRNN(input_size, hidden_size, num_layers, output_size)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for i, (inputs, labels) in enumerate(train_dataloader):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Check the original shape of the inputs (batch_size, sequence_length, input_size)\n",
    "        # You may need to adjust the permutation dimensions based on your actual input shape\n",
    "        #inputs = inputs.squeeze(-4,-3)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{EPOCHS}], Step [{i+1}/{len(train_dataloader)}], Loss: {loss.item():.4f}')\n",
    "\n",
    "\n",
    "\n",
    "# Test the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for inputs, labels in test_dataloader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Test Accuracy: {accuracy:.2f}%')\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), 'rnn_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab9ef15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2233fb-20c5-47b2-b19f-30821ace8fa3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word_id</th>\n",
       "      <th>hapana</th>\n",
       "      <th>kumi</th>\n",
       "      <th>mbili</th>\n",
       "      <th>moja</th>\n",
       "      <th>nane</th>\n",
       "      <th>ndio</th>\n",
       "      <th>nne</th>\n",
       "      <th>saba</th>\n",
       "      <th>sita</th>\n",
       "      <th>tano</th>\n",
       "      <th>tatu</th>\n",
       "      <th>tisa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id_jp2pxl0r84ya.wav</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.998971</td>\n",
       "      <td>2.764155e-07</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id_ndduqqvthbpx.wav</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000360</td>\n",
       "      <td>1.296941e-06</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.999546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id_36oxymxfcm6q.wav</td>\n",
       "      <td>0.007725</td>\n",
       "      <td>0.003506</td>\n",
       "      <td>0.020147</td>\n",
       "      <td>0.011722</td>\n",
       "      <td>0.004154</td>\n",
       "      <td>0.019079</td>\n",
       "      <td>0.003372</td>\n",
       "      <td>0.032276</td>\n",
       "      <td>0.084893</td>\n",
       "      <td>2.653792e-03</td>\n",
       "      <td>0.011227</td>\n",
       "      <td>0.799244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id_ue9b0to760pg.wav</td>\n",
       "      <td>0.000437</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>9.993451e-01</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id_prja4oprb914.wav</td>\n",
       "      <td>0.000146</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.998558</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000489</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>1.928133e-04</td>\n",
       "      <td>0.000178</td>\n",
       "      <td>0.000061</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Word_id    hapana      kumi     mbili      moja      nane  \\\n",
       "0  id_jp2pxl0r84ya.wav  0.000004  0.000002  0.000001  0.000016  0.000007   \n",
       "1  id_ndduqqvthbpx.wav  0.000009  0.000002  0.000004  0.000006  0.000013   \n",
       "2  id_36oxymxfcm6q.wav  0.007725  0.003506  0.020147  0.011722  0.004154   \n",
       "3  id_ue9b0to760pg.wav  0.000437  0.000016  0.000016  0.000002  0.000039   \n",
       "4  id_prja4oprb914.wav  0.000146  0.000044  0.000065  0.000065  0.998558   \n",
       "\n",
       "       ndio       nne      saba      sita          tano      tatu      tisa  \n",
       "0  0.000045  0.000014  0.000003  0.998971  2.764155e-07  0.000006  0.000930  \n",
       "1  0.000006  0.000013  0.000004  0.000360  1.296941e-06  0.000036  0.999546  \n",
       "2  0.019079  0.003372  0.032276  0.084893  2.653792e-03  0.011227  0.799244  \n",
       "3  0.000026  0.000002  0.000062  0.000003  9.993451e-01  0.000035  0.000016  \n",
       "4  0.000040  0.000489  0.000071  0.000090  1.928133e-04  0.000178  0.000061  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create submission DataFrame\n",
    "class_mapping = ['hapana',\n",
    "                  'kumi',\n",
    "                  'mbili',\n",
    "                  'moja',\n",
    "                  'nane',\n",
    "                  'ndio',\n",
    "                  'nne',\n",
    "                  'saba',\n",
    "                  'sita',\n",
    "                  'tano',\n",
    "                  'tatu',\n",
    "                  'tisa']\n",
    "\n",
    "test = pd.read_csv('data/Test.csv')\n",
    "submission = pd.DataFrame({'Word_id': test['Word_id']})\n",
    "for i, label in enumerate(class_mapping):\n",
    "    submission[label] = predicted[:,i].numpy()\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16e0bdf-79c4-4afb-819c-690c3e5f490c",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('data/submission_torch_resnet18_normal-spec_12epoch_lrschedule.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08cfa3e4-1f18-4879-a5c0-231fdb59a8b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
